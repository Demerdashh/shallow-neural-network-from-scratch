# ğŸ§  Shallow Neural Network from Scratch â€” Planar Data Classification

This project is a full implementation of a **2-layer neural network from scratch**, using **only NumPy**. It classifies **moons data** into two classes by manually building the entire training pipeline: initialization, forward/backward propagation, cost calculation, and gradient descent.

> **Inspired by** [deeplearning.aiâ€™s Neural Networks and Deep Learning (Week 3)](https://www.coursera.org/learn/neural-networks-deep-learning) by Andrew Ng.

---

## ğŸš€ Features

- âœ… **Pure NumPy** â€” No ML libraries or frameworks used  
- âœ… Manual implementation of:
  - Forward propagation (ReLU + Sigmoid)
  - Cost computation (binary cross-entropy)
  - Backward propagation
  - Gradient descent optimization
- âœ… Vectorized implementation for efficiency
- âœ… Custom prediction and accuracy calculation
- âœ… Decision boundary visualization

---

## ğŸ—‚ï¸ Folder Structure

<img width="321" height="516" alt="image" src="https://github.com/user-attachments/assets/19606554-a1a5-47e7-8ec8-d12141bde357" />


---

## ğŸ§ª Dataset

- **Dataset**: moons dataset from `sklearn.datasets.make_moons()` 
- **Preprocessing**:
  - Normalized inputs
  - Binary labels (0 or 1)
  - Visualization to inspect decision boundary

---

## ğŸ“œ Methodology

This project is built fully from scratch â€” no `Keras`, no `scikit-learn`, no black-box training.

### ğŸ”„ Preprocessing
- Generated or loaded 2D binary classification dataset
- Scaled and visualized

### ğŸ§® Model Architecture
- **Input layer**: 2 features (X1, X2)
- **Hidden layer**: Fully connected, with tanh activation
- **Output layer**: Sigmoid activation (for binary classification)

### ğŸ§  Training Pipeline
- Parameter initialization (He initialization)
- Forward pass (linear â†’ tanh â†’ linear â†’ sigmoid)
- Compute cost using binary cross-entropy
- Backward propagation (manual gradients for tanh and sigmoid)
- Update parameters using gradient descent

### ğŸ“ˆ Evaluation
- Accuracy computed using custom logic
- Decision boundary visualized using `matplotlib`

---

## âŒ No Machine Learning Libraries Used

| Library        | Used? |
|----------------|-------|
| NumPy          | âœ…     |
| scikit-learn   | âŒ     |
| TensorFlow     | âŒ     |
| Keras          | âŒ     |
| PyTorch        | âŒ     |

All logic is hand-coded to solidify understanding of forward and backward propagation.

---

## ğŸ“Š Sample Output

- Accuracy: **97.5%** on non-linearly separable data   
- Clear nonlinear decision boundary plotted

---

## ğŸ’¡ Inspiration

This project is based on:

> ğŸ“ **deeplearning.ai â€“ Neural Networks and Deep Learning (Week 3)**  
> by [Andrew Ng](https://www.andrewng.org/)

Additional self-imposed challenges:
- Used manual `matplotlib`-based decision boundary plotting  
- No reliance on any machine learning libraries  
- Used real activation gradients (tanh, sigmoid) implemented from scratch

---

## ğŸ§‘â€ğŸ’» Author

**Youssef Eldemerdash**  
_Learning deeply by building everything from scratch._

---

## ğŸ“„ License

This project is licensed under the MIT License. See [LICENSE](./LICENSE) for more details.

